# üçΩÔ∏è TypeScript Recipe Scraper & Processing Service

[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=for-the-badge&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![Node.js](https://img.shields.io/badge/Node.js-43853D?style=for-the-badge&logo=node.js&logoColor=white)](https://nodejs.org/)
[![Supabase](https://img.shields.io/badge/Supabase-3ECF8E?style=for-the-badge&logo=supabase&logoColor=white)](https://supabase.com/)
[![Vercel](https://img.shields.io/badge/Vercel-000000?style=for-the-badge&logo=vercel&logoColor=white)](https://vercel.com/)

**A production-ready, enterprise-grade TypeScript service for intelligent recipe scraping, OCR processing, ingredient linking, and comprehensive enrichment with robust validation and quality assurance.**

## üéâ **LATEST UPDATES (November 2025)**

### ‚úÖ **NEW: Multi-Modal Social Media Scraping**
- **Production-Ready Video Scraping**: Full TikTok, Instagram, YouTube support using yt-dlp
- **Parallel Processing**: OCR + Audio Transcription running simultaneously
- **AI-Powered Parsing**: Smart two-tier NLP with Gemini/GPT-4/Claude fallback
- **Cost-Optimized**: ~$0.005 per difficult recipe, free for most content

### üöÄ **Key Improvements**
1. **Media Downloader** (`src/utils/mediaDownloader.ts`)
   - Downloads from 1000+ platforms via yt-dlp
   - Automatic audio extraction & thumbnail capture
   - Built-in cleanup and resource management

2. **Enhanced NLP Parser** (`src/enrichment/nlpRecipeParser.ts`)
   - Local-first parsing (free, instant)
   - AI fallback for complex text (accurate, ~$0.001/recipe)
   - Supports Gemini, OpenAI, and Anthropic

3. **Fully-Wired Social Media Scrapers**
   - `UniversalRecipeScraper` now calls all processors
   - Frame-by-frame video OCR
   - Speech-to-text audio transcription
   - Intelligent text combination & parsing

## üöÄ **PRODUCTION-READY FEATURES**

### **üîç Multi-Platform Recipe Scraping**
- **Website Scraping**: Universal recipe extraction from 1000+ recipe sites
- **Social Media Integration**: ‚úÖ **FULLY IMPLEMENTED** - TikTok, Instagram, YouTube with multi-modal processing
- **OCR Processing**: Video text extraction with Google Vision API + Tesseract.js fallback
- **Audio Transcription**: Whisper/Google Speech API for voice recipe extraction
- **Robust Error Handling**: Comprehensive retry logic and graceful degradation

### **üß† Intelligent Processing Pipeline** 
- **Advanced NLP Parsing**: Multi-model ingredient extraction using compromise, natural, node-nlp
- **Ingredient Linking**: Robust linking to master ingredients database with fuzzy matching
- **Validation Notes**: Centralized quality tracking and debugging system
- **AI Enrichment**: Google Gemini-powered recipe enhancement and completion

### **üî¨ Comprehensive Data Quality**
- **Nutrition Calculation**: USDA FoodData Central API integration
- **Diet Suitability Analysis**: 20+ dietary restrictions detection
- **Quality Scoring**: Data completeness and parsing confidence metrics
- **Schema Validation**: TypeScript-enforced data integrity

### **‚ö° Performance & Scalability**
- **Unified Crawler Architecture**: Consolidated, modular crawler system following SOLID/DRY/KISS principles
- **Real-time Error Adaptation**: Dynamic site configuration adjustments to prevent blocking
- **Batch Processing**: Concurrent processing with intelligent rate limiting and retry logic
- **Memory Efficiency**: Stream-based processing for large datasets

## üèóÔ∏è **UNIFIED CRAWLER ARCHITECTURE** 

### **üì¶ Modular Design (NEW)**

The crawler system has been **fully consolidated** into a modular, reusable architecture:

```
src/
‚îú‚îÄ‚îÄ shared/
‚îÇ   ‚îú‚îÄ‚îÄ crawlerConstants.ts    # Single source of truth for all crawler config
‚îÇ   ‚îî‚îÄ‚îÄ crawlerHelpers.ts      # Reusable functions (CSV parsing, URL generation, etc.)
‚îú‚îÄ‚îÄ crawler/
‚îÇ   ‚îî‚îÄ‚îÄ UnifiedCrawler.ts      # Main crawler logic with real-time adaptation
‚îî‚îÄ‚îÄ unified-crawler.ts         # CLI entry point with Commander.js
```

### **üéØ Key Architectural Benefits**

- **Single Responsibility**: Each module has a focused, well-defined purpose
- **DRY Principle**: Shared constants and helpers eliminate code duplication
- **SOLID Design**: Modular, extensible, and maintainable codebase
- **KISS Implementation**: Simplified interface replacing complex scattered logic
- **Type Safety**: Full TypeScript typing throughout with strict error handling

### **‚ö° Performance Features**

- **Real-time Error Adaptation**: Automatically adjusts delays, concurrency, and retries based on site responses
- **Site-specific Configuration**: Dynamic per-domain rate limiting and blocking
- **Comprehensive Error Tracking**: Statistics-based adaptive blocking for problematic domains
- **Multiple Crawler Modes**: Test, sample, full, and adaptive crawling with flexible options

### **‚ö° Performance & Scalability (Legacy)**
- **Intelligent Caching**: 95%+ cache hit rates for common ingredients
- **Batch Processing**: High-throughput recipe processing capabilities
- **Resource Optimization**: Memory-efficient processing with timeout handling
- **Production Monitoring**: Comprehensive logging and error tracking

## üîß **ENVIRONMENT SETUP**

### **Required Environment Variables**

Create a `.env` file in the project root with the following variables:

```bash
# Database Configuration
SUPABASE_URL=your_supabase_project_url
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key
SUPABASE_ANON_KEY=your_supabase_anon_key
DATABASE_URL=your_database_connection_string

# AI & NLP Services
GOOGLE_API_KEY=your_google_api_key
GOOGLE_VISION_API_KEY=your_google_vision_api_key  # For OCR processing
GOOGLE_VISION_KEY_FILE=path/to/google-vision-service-account.json

# Nutrition & Food Data
USDA_API_KEY=your_usda_fooddata_central_api_key

# Image Processing
IMGUR_CLIENT_ID=your_imgur_client_id

# Performance & Features
NO_FILE_CACHE=true
LITE_MODEL=gemini-1.5-flash-latest  # Optional: AI model selection
MAX_RETRIES=3

# NEW: AI Parsing (Optional - for enhanced accuracy on difficult text)
GOOGLE_API_KEY=your_google_gemini_key  # Recommended (cheapest)
OPENAI_API_KEY=your_openai_key         # Alternative
ANTHROPIC_API_KEY=your_anthropic_key   # Alternative

# NEW: Audio Transcription (Optional - for video recipes)
ENABLE_AUDIO_TRANSCRIPTION=true        # Set to true to enable
GOOGLE_CLOUD_SPEECH_API_KEY=your_key  # For Google Speech-to-Text
# OR use OPENAI_API_KEY for Whisper API

# NEW: Advanced OCR (Optional - for better video text extraction)
GOOGLE_VISION_API_KEY=your_vision_key  # Fallback to Tesseract if not set
TIMEOUT_MS=30000
CACHE_ENABLED=true
```

### **API Key Setup Instructions**

#### **1. Supabase Setup**
```bash
# 1. Create project at https://supabase.com
# 2. Navigate to Settings > API
# 3. Copy your URL and service role key
```

#### **2. Google AI & Vision API**
```bash
# 1. Visit https://console.cloud.google.com
# 2. Enable Gemini API and Vision API
# 3. Create service account and download JSON key
# 4. Generate API keys for both services
```

#### **3. USDA FoodData Central**
```bash
# 1. Register at https://fdc.nal.usda.gov/api-key-signup.html
# 2. Receive API key via email
# 3. Add to environment variables
```

#### **4. NEW: System Dependencies (Required for Social Media Scraping)**

**Install yt-dlp (video downloader):**
```bash
# macOS
brew install yt-dlp

# Linux (Ubuntu/Debian)
sudo apt update && sudo apt install yt-dlp

# Linux (alternative - via pip)
pip install yt-dlp

# Windows
pip install yt-dlp
# OR download from: https://github.com/yt-dlp/yt-dlp/releases
```

**Install ffmpeg (audio/video processing):**
```bash
# macOS
brew install ffmpeg

# Linux (Ubuntu/Debian)
sudo apt update && sudo apt install ffmpeg

# Windows
# Download from: https://ffmpeg.org/download.html
# OR use Chocolatey: choco install ffmpeg
```

**Verify installation:**
```bash
yt-dlp --version  # Should show version number
ffmpeg -version   # Should show version number
```

#### **5. Imgur API (Optional)**
```bash
# 1. Create account at https://imgur.com
# 2. Register application at https://api.imgur.com/oauth2/addclient
# 3. Copy Client ID
```

## üöÄ **LOCAL DEVELOPMENT**

### **Prerequisites**
- Node.js 18+ and pnpm
- TypeScript 5.0+
- Git
- Required API keys (see Environment Setup)

### **Quick Start**

```bash
# 1. Clone repository
git clone https://github.com/your-username/typescript_scraper_service.git
cd typescript_scraper_service

# 2. Install dependencies
pnpm install

# 3. Set up environment variables
cp .env.example .env
# Edit .env with your API keys

# 4. Run TypeScript compilation
pnpm run build

# 5. Start development server
pnpm run dev:api

# 6. Test the service
curl -X POST http://localhost:3000/api/scrape \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example-recipe-site.com/recipe"}'
```

### **Available Scripts**

```bash
# Core Application
pnpm run build          # Compile TypeScript to JavaScript
pnpm run dev:api        # Start development server
pnpm run start:api      # Start production server
pnpm run test           # Run test suite

# Unified Crawler (NEW - Consolidated Architecture)
node unified-crawler.js --mode test --sites 5        # Test crawler on 5 sites
node unified-crawler.js --mode sample --count 10     # Sample 10 URLs from each site
node unified-crawler.js --mode full --batch-size 50  # Full crawl with batch processing
node unified-crawler.js --mode adaptive              # Adaptive crawl with error handling

# Legacy Scripts (Deprecated - Use Unified Crawler Above)
pnpm run crawl          # [DEPRECATED] Use unified crawler instead
pnpm run crawl:all      # [DEPRECATED] Use unified crawler instead
pnpm run discover:all   # [DEPRECATED] Use unified crawler instead
pnpm run mass-scrape    # [DEPRECATED] Use unified crawler instead
```

### **üéÆ Unified Crawler Usage Examples**

```bash
# Test mode - Quick validation on a few sites
node unified-crawler.js --mode test --sites 5 --max-urls 2

# Sample mode - Get sample URLs from each site
node unified-crawler.js --mode sample --count 10 --output sample-results.json

# Full crawl with batch processing
node unified-crawler.js --mode full --batch-size 50 --concurrency 3

# Adaptive mode with real-time error handling
node unified-crawler.js --mode adaptive --csv data/Data.csv --max-retries 5

# Specific site testing
node unified-crawler.js --mode test --url "https://example.com" --timeout 30000
```

### **üìä Crawler Options**

- `--mode`: Crawler mode (`test`, `sample`, `full`, `adaptive`)
- `--sites`: Number of sites to process (test mode)
- `--count`: Number of URLs to sample per site (sample mode)
- `--batch-size`: Batch processing size (full mode)
- `--concurrency`: Maximum concurrent requests (1-10)
- `--csv`: CSV file path for website data
- `--output`: Output file path for results
- `--timeout`: Request timeout in milliseconds
- `--max-retries`: Maximum retry attempts per request
- `--verbose`: Enable detailed logging

## üåê **PRODUCTION DEPLOYMENT**

### **Vercel Deployment (Recommended)**

```bash
# 1. Install Vercel CLI
npm install -g vercel

# 2. Login to Vercel
vercel login

# 3. Deploy to Vercel
vercel --prod

# 4. Set environment variables in Vercel dashboard
# Go to your project settings and add all required environment variables
```

### **Alternative Deployment Options**

#### **Docker Deployment**

**Option 1: Docker Compose (Recommended)**
```bash
# 1. Set up environment variables
cp .env.example .env
# Edit .env with your API keys

# 2. Build and run with docker-compose
docker-compose up --build

# 3. Test the service
curl -X POST http://localhost:3000/api/scrape \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example-recipe-site.com/recipe"}'
```

**Option 2: Manual Docker Build**
```bash
# Build image
docker build -t recipe-scraper .

# Run container with environment file
docker run -p 3000:3000 --env-file .env recipe-scraper

# Or run with individual environment variables
docker run -p 3000:3000 \
  -e SUPABASE_URL=your_url \
  -e SUPABASE_SERVICE_ROLE_KEY=your_key \
  -e GOOGLE_API_KEY=your_key \
  recipe-scraper
```

#### **Railway/Render/Heroku**
```bash
# Build command: npm run build
# Start command: npm start
# Node.js version: 18+
```

### **‚ö†Ô∏è CRITICAL: Social Media Scraping Dependencies**

**TikTok & Instagram Scraping Requirements:**
- Uses `yt-dlp` (Python CLI tool) for video processing
- Requires Python runtime + yt-dlp binary in production environment
- **Vercel Limitation**: Standard Node.js runtime doesn't include Python/yt-dlp

**Recommended Solutions:**
1. **External Microservice** (Recommended): Deploy yt-dlp service separately (Docker + Cloud Run/Fargate)
2. **Custom Runtime**: Use Vercel functions with custom Python layer
3. **Alternative Platform**: Use Railway/Render with full Python support

**YouTube Scraping**: ‚úÖ Works on all platforms (pure Node.js)
**Website Scraping**: ‚úÖ Works on all platforms (pure Node.js)

## Local Development

### Standalone Scraper

A standalone scraper script is available for local development and GitHub Actions automation:

```bash
pnpm standalone
```

This script:
- Scrapes recipes from predefined URLs using the `recipe-scraper` package
- Saves recipes directly to your Supabase database
- Avoids circular dependencies and complex imports
- Runs as a simple Node.js script without requiring TypeScript compilation

#### Environment Variables for Standalone Scraper

The standalone scraper requires the following environment variables:

```json
{
  "url": "https://example-recipe-site.com/recipe",
  "recipe_id_override": null,
  "include_nutrition": true,
  "include_gemma_enrichment": true,
  "include_image_processing": true,
  "enable_ocr": true,
  "validate_quality": true
}
```

#### **Response Format**
```json
{
  "success": true,
  "data": {
    "id": "uuid-here",
    "title": "Amazing Recipe Name",
    "description": "Recipe description",
    "ingredients": [
      {
        "id": "ingredient-uuid",
        "name": "2 cups all-purpose flour",
        "clean_name": "flour",
        "quantity": [2],
        "unit": "cup",
        "category": "Baking & Grains",
        "grams": 240,
        "validation_notes": []
      }
    ],
    "instructions": [
      {
        "step_number": 1,
        "instruction": "Preheat oven to 350¬∞F",
        "timing": {"minutes": 5},
        "equipment": ["oven"],
        "ingredients_mentioned": []
      }
    ],
    "nutrition": {
      "calories": 250,
      "protein_g": 8.5,
      "carbs_g": 45.2,
      "fat_g": 5.1
    },
    "metadata": {
      "source_url": "original-url",
      "scraping_method": "website",
      "completeness_score": 0.85,
      "parsing_confidence": 0.92,
      "processing_time_ms": 3420
    }
  },
  "validation_notes": [
    "Successfully enriched 8 ingredients",
    "OCR extracted 15 words from video frames"
  ]
}
```

### **Batch Processing Endpoint**

**POST** `/api/batch-scrape`

```json
{
  "urls": [
    "https://recipe1.com",
    "https://recipe2.com"
  ],
  "options": {
    "include_nutrition": true,
    "max_concurrent": 5
  }
}
```

### **Health Check Endpoint**

**GET** `/api/health`

```json
{
  "status": "healthy",
  "version": "1.0.0",
  "services": {
    "database": "connected",
    "google_vision": "available",
    "usda_api": "available"
  }
}
```

### **Troubleshooting**

* **Error 400: Bad Request**: Ensure the request body is properly formatted and all required fields are present.
* **Error 401: Unauthorized**: Verify that the API key is valid and correctly configured.
* **Error 429: Too Many Requests**: Check the rate limit for your API key and adjust your request frequency accordingly.
* **Error 500: Internal Server Error**: Contact support for assistance with resolving server-side issues.

### **API Usage Examples**

* **Scrape a single recipe**: `curl -X POST https://example.com/api/scrape -H "Content-Type: application/json" -d '{"url": "https://example-recipe-site.com/recipe"}'`
* **Batch scrape multiple recipes**: `curl -X POST https://example.com/api/batch-scrape -H "Content-Type: application/json" -d '{"urls": ["https://recipe1.com", "https://recipe2.com"]}'`
* **Check API health**: `curl -X GET https://example.com/api/health`
